{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader('https://news.ycombinator.com/item?id=36697119')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endisneigh 9 months ago  \n",
      "             | next [–] \n",
      "\n",
      "I wish the folks who clearly do not like Google would just not use their products instead of spamming every thread about how they will kill the product, true or not.——Anyway,It’s not clear which model they’re using for this. I assume whatever Bard is using, but who knows. This is relevant because depending on the intended experience the latency will matter.Overall it’s not a bad idea, but I do wonder what the monetization path will be for Google. I imagine this will be part of workspace. Perhaps they will add more tiers to include these offerings.I wish they shared a bit about how this will be differentiated from Bard. Is this simply a new front end to Bard? It’s really an open question. I haven’t seen many products that use LLMs that are better than the prompt response UX.The most interesting thing about this blog post is the “source grounding.” I’m curious if there’s actual engineering behind it, or is it prompt tweaking contextualized behind the scenes on a given doc.\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = HumanMessagePromptTemplate.from_template('Please give me a short summary of the following HackerNews comment:\\n {comment}')\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Suriya\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "request = chat_prompt.format_prompt(comment = data[0].page_content).to_messages()\n",
    "\n",
    "\n",
    "res = model(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The commenter expresses frustration with individuals who criticize Google on HackerNews threads. They question the model Google is using for their new product and speculate on potential monetization strategies. They wonder how the product will be differentiated from Bard and express interest in the concept of \"source grounding.\" They are curious about the engineering behind it and whether it involves prompt tweaking contextualized on a given document.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
